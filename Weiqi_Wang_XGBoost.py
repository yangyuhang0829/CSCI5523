# -*- coding: utf-8 -*-
"""instacart_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XIlQAsto1j4P9LuD_IStlsOSIvtz-Dzj

# Import packages and dataset
"""

# import packages
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# garbage collector
import gc
gc.enable()

from google.colab import drive
drive.mount('/content/drive')

# import datasets
aisles = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/aisles.csv')
departments = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/departments.csv')
order_products_prior = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/order_products__prior.csv')
order_products_train = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/order_products__train.csv')
orders = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/orders.csv')
products = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/products.csv')

aisles.head()

departments.head()

orders.head()

products.head()

order_products_prior.head()

order_products_train.head()

"""# Feature Engineering"""

# create a dataframe with all prior user_order_product information
prior = orders.merge(right=order_products_prior, on='order_id', how='inner')
prior.head()

# delete order_products_prior
del order_products_prior
gc.collect()

"""## Feature engineering of user information"""

# feature engineering on user info
# calculate the total number of orders placed by each user
user_info = prior.groupby('user_id')['order_number'].max().reset_index(name='user_total_orders')
user_info.head()

# calculate the total number of products bought by each user
temp = prior.groupby('user_id')['product_id'].count().reset_index(name='user_total_products')
user_info = user_info.merge(temp, on='user_id', how='left')
user_info.head()

# calculate the reorder times of each user
# temp = prior.groupby('user_id')['reordered'].sum().reset_index(name='user_reorder_times')
# user_info = user_info.merge(temp, on='user_id', how='left')
# user_info.head()

# calculate the reorder ratio of each user
temp = prior.groupby('user_id')['reordered'].mean().reset_index(name='user_reorder_ratio')
user_info = user_info.merge(temp, on='user_id', how='left')
user_info.head()

# calculate the average day of week of each user
temp = prior.groupby('user_id')['order_dow'].mean().reset_index(name='user_avg_order_dow')
user_info = user_info.merge(temp, on='user_id', how='left')
user_info.head()

# calculate the average order hour of each user
temp = prior.groupby('user_id')['order_hour_of_day'].mean().reset_index(name='user_avg_order_hour')
user_info = user_info.merge(temp, on='user_id', how='left')
user_info.head()

# calculate the average time interval of two orders of each user
temp = prior.groupby('user_id')['days_since_prior_order'].mean().reset_index(name='user_avg_order_interval')
user_info = user_info.merge(temp, on='user_id', how='left')
user_info.head()

"""## Feature engineering of product information"""

# calculate the total number of times each product has been ordered
pd_info = prior.groupby('product_id')['order_id'].count().reset_index(name='pd_total_ordered_times')
pd_info.head()

# calculate the total reorder times of each product
# temp = prior.groupby('product_id')['reordered'].sum().reset_index(name='pd_total_reorder_times')
# pd_info = pd_info.merge(temp, on='product_id', how='left')
# pd_info.head()

# calculate the total reorder ratio of each product
temp = prior.groupby('product_id')['reordered'].mean().reset_index(name='pd_total_reorder_ratio')
pd_info = pd_info.merge(temp, on='product_id', how='left')
pd_info.head()

# calculate the average add to cart order of each product
temp = prior.groupby('product_id')['add_to_cart_order'].mean().reset_index(name='pd_avg_add_to_cart_order')
pd_info = pd_info.merge(temp, on='product_id', how='left')
pd_info.head()

"""## Feature engineering of user_product information

### The number of times, average add to cart order of each product of each user
"""

# Feature engineering of user_product information
# The number of times each product has been bought by each user
user_pd_info = prior.groupby(['user_id', 'product_id'])['order_id'].count().reset_index(name='user_pd_total_order_times')
user_pd_info.head()

# calculate the average add to cart order of each product of each user
temp = prior.groupby(['user_id','product_id'])['add_to_cart_order'].mean().reset_index(name='user_pd_avg_add_to_cart_order')
user_pd_info = user_pd_info.merge(temp, how='left')
user_pd_info.head()

"""### The number of times each product has been bought by each user of his/her last 5 orders"""

# reverse order number
prior['order_number_reversed'] = prior.groupby('user_id')['order_number'].transform(max) - prior.order_number + 1
prior.head()

# using the last five orders of each user
last_five_orders = prior.loc[prior['order_number_reversed'] <= 10]
last_five_orders.head()

temp = last_five_orders.groupby(['user_id','product_id'])['order_id'].count().reset_index(name='user_pd_last_5_order_times')
user_pd_info = user_pd_info.merge(temp, how='left')
user_pd_info.head()

"""### The reorder ratio of each product of each user"""

# calculate the reorder time of each product of each user
temp = prior.groupby(['user_id','product_id'])['reordered'].sum().reset_index(name='user_pd_reorder_time')
user_pd_info = user_pd_info.merge(temp, how='left')
user_pd_info.head()

# The time when the user ordered the product for the first time
temp = prior.groupby(['user_id','product_id'])['order_number'].min().reset_index(name='first_order_number')
user_pd_info = user_pd_info.merge(temp, how='left')
temp = prior.groupby('user_id')['order_number'].max().reset_index()
user_pd_info = user_pd_info.merge(temp, how='left')
user_pd_info.head()

# calculate the order number from the first time ordering the item to the last time ordering the item
user_pd_info['pd_first_to_last_range'] = user_pd_info.order_number - user_pd_info.first_order_number + 1

# calculate the reorder ratio of each product of each user
user_pd_info['user_pd_reorder_ratio'] = user_pd_info.user_pd_total_order_times / user_pd_info.pd_first_to_last_range
user_pd_info.head()

# drop unwanted columns
user_pd_info = user_pd_info.drop(columns=['first_order_number','order_number','pd_first_to_last_range', 'user_pd_reorder_time'])
user_pd_info.head()

"""### The difference between the total order numbers of a user and the last order time of a product of a user"""

# calculate the difference between the total order numbers of a user and the last order time of a product of a user
temp = prior.groupby('user_id')['order_number'].max().reset_index(name='total_order_number')
user_pd_info = user_pd_info.merge(temp, how='left')
temp = prior.groupby(['user_id','product_id'])['order_number'].max().reset_index(name='last_order_number')
user_pd_info = user_pd_info.merge(temp, how='left')
user_pd_info['user_pd_diff_tot_last'] = user_pd_info.total_order_number - user_pd_info.last_order_number + 1
user_pd_info.head()

# drop unwanted columns
user_pd_info = user_pd_info.drop(columns=['last_order_number','total_order_number'])
user_pd_info.head()

"""## Merge feature engineering dataframes"""

# merge user_pd_info and user_info
prior = user_pd_info.merge(user_info, on='user_id', how='left')
# merge prior and pd_info
prior = prior.merge(pd_info, on='product_id', how='left')
prior.head(10)

# delete user_pd_info
del [user_info, pd_info, user_pd_info, temp]
gc.collect()

prior.shape

"""# Train-Test Split"""

# create a dataframe with all train user_order_product information
train = orders.merge(right=order_products_train, on='order_id', how='inner')
train.head()

# delete order_products_train
del order_products_train
gc.collect()

# merge prior and train
data = prior.merge(train[['user_id','product_id','reordered']], on=['user_id','product_id'], how='left')
# merge product to add aisle and department information
data = data.merge(products[['product_id', 'aisle_id', 'department_id']], on='product_id', how='left')
data.head(10)

# add eval_set info
orders_train_test = orders.loc[ ((orders['eval_set'] == 'train') | (orders['eval_set'] == 'test')), ['user_id','order_id','eval_set']]
orders_train_test.head()

# merge orders_train_test and data
data = data.merge(orders_train_test, on='user_id',how='left')
data.head()

# delete orders_train_test, prior, train
del [orders_train_test, prior, train]
gc.collect()

# train-test split
train = data[data['eval_set'] == 'train']
test = data[data['eval_set'] == 'test']

# delete data
del data
gc.collect()

# fill na in reordered with 0
train.loc[:,'reordered'] = train['reordered'].fillna(0)
# delete unrelated columns
train = train.drop(columns=['eval_set', 'order_id'])
test = test.drop(columns=['eval_set', 'order_id'])

# set index
# train = train.set_index(['user_id','product_id'])
# test = test.set_index(['user_id','product_id'])

# mean encoding since xgboost cannot automatically handle categorical data
columns = ['aisle_id', 'department_id']
for col in columns:
  means = train.groupby(col).reordered.mean()
  train[col] = train[col].map(means)
  test[col] = test[col].map(means)

"""# XGBoost

## tuning parameters
"""

# split validation set
X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['user_id','product_id','reordered']),
                                                  train['reordered'], 
                                                  test_size=0.9, random_state=42)

# tuning parameter
param_test1 = {
 'max_depth':range(3,10,2),
 'min_child_weight':range(1,6,2)
}

gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,
 min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
 objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
 param_grid = param_test1, scoring='f1',n_jobs=4,iid=False, cv=5)
gsearch1.fit(X_train,y_train)
# gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_

gsearch1.cv_results_

"""## Setting parameters"""

# set parameters
xgb_parameters = {
    'objective' : 'binary:logistic',
    'eval_metric' : 'logloss',
    'max_depth' : 5,
    'min_child_weight': 3,
    'colsample_bytree' : 0.4,
    'subsample' : 0.8,
    'num_boost_round' : 10
}

# initial the model
xgb = xgb.XGBClassifier(parameters=xgb_parameters)

"""## Training and validation"""

# split validation set
X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['user_id','product_id','reordered']),
                                                  train['reordered'], 
                                                  test_size=0.3, random_state=42)

# fit the model
xgb.fit(X_train, y_train)

# make prediction
y_pred = (xgb.predict_proba(X_val)[:, 1] >= 0.21).astype('int') #setting a threshold

!pip install scikit-plot

# evaluation
from sklearn.metrics import f1_score, classification_report
from scikitplot.metrics import plot_confusion_matrix
from scikitplot.classifiers import plot_feature_importances
print('F1 Score: {}'.format(f1_score(y_pred, y_val)))
print(classification_report(y_pred, y_val))
# plot confusion matrix
plot_confusion_matrix(y_pred, y_val)

# plot importance
features = train.drop(columns=['user_id','product_id','reordered'])
plot_feature_importances(xgb, feature_names=features.columns, x_tick_rotation=90, max_num_features=20, figsize=(10,8))

# delete X_train, X_test, y_train, y_test
del [X_train, X_val, y_train, y_val, means]
gc.collect()

"""## Fit entire training set"""

# fit on entire dataset
xgb.fit(train.drop(columns=['user_id','product_id','reordered','user_avg_order_dow','user_avg_order_hour']), train['reordered'])

# make prediction on test set
test['reordered'] = (xgb.predict_proba(test.drop(columns=['user_id','product_id','reordered','user_avg_order_dow','user_avg_order_hour']))[:, 1] >= 0.21).astype('int') # setting a threshold
test.head()

"""## Create submission file"""

# create submission file
submission = test[['user_id','product_id','reordered']]
# join orders dataset where eval_set == test
test_idx = orders[orders['eval_set'] == 'test'].index
submission = submission.merge(orders.iloc[test_idx][['user_id', 'order_id']], on='user_id', how='left')
submission = submission.drop(columns=['user_id'])
submission.head()

# sort and drop unwanted rows
submission = submission.sort_values(by=['order_id','product_id'])
submission = submission[submission['reordered'] == 1]
submission.head()

submission['product_id'] = submission.product_id.astype(str)
submission = submission.groupby('order_id')['product_id'].apply(' '.join).reset_index(name='products')
submission.head()

# import sample submission file to add unpresented orders
sample = pd.read_csv('/content/drive/My Drive/instacart-market-basket-analysis/sample_submission.csv', usecols=['order_id'])

# merge submission and sample submission
submission = sample.merge(submission, on='order_id', how='left')
# fill na with None
submission['products'] = submission['products'].fillna('None')
# submission = submission.rename(columns={'product_id': 'products'})
submission.head()

submission.to_csv('/content/drive/My Drive/instacart-market-basket-analysis/submission.csv', index=False, header=True)

